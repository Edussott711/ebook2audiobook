# Support CPU et GPU - Mode Distribu√©

## üéØ Vue d'ensemble

Le mode distribu√© supporte **√† la fois CPU et GPU** sur les workers. Vous pouvez mixer les deux types dans le m√™me cluster !

## üîç D√©tection Automatique

### Code de D√©tection

**Fichier** : `lib/distributed/tasks.py`

```python
def _is_gpu_available() -> bool:
    """V√©rifie si GPU est disponible."""
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False

def _get_or_create_tts_engine(tts_config: Dict[str, Any]):
    """Cr√©e TTS engine avec auto-d√©tection GPU/CPU."""
    requested_device = tts_config.get('device')
    gpu_available = _is_gpu_available()

    # Auto-d√©tection
    if requested_device:
        device = requested_device  # Explicite
    else:
        device = 'cuda' if gpu_available else 'cpu'  # Auto

    # Fallback si GPU demand√© mais pas dispo
    if device == 'cuda' and not gpu_available:
        logger.warning("GPU requested but not available, falling back to CPU")
        device = 'cpu'

    # Cr√©er TTS avec device appropri√©
    tts_manager = TTSManager(..., device=device)
    return tts_manager
```

### Workflow de D√©tection

```
Worker d√©marre
    ‚Üì
V√©rifier torch.cuda.is_available()
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GPU Disponible ‚îÇ Pas de GPU      ‚îÇ
‚îÇ  ‚úì CUDA active  ‚îÇ ‚úì CPU seulement ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                 ‚îÇ
         ‚ñº                 ‚ñº
   device = 'cuda'   device = 'cpu'
         ‚îÇ                 ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚ñº
          TTS model charg√©
         avec device appropri√©
```

---

## üöÄ D√©ploiement GPU

### Option 1: Auto-d√©tection (Recommand√©)

Le worker d√©tecte automatiquement le GPU :

```bash
# Script d√©tecte et utilise GPU automatiquement
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
```

### Option 2: GPU Explicite

Forcer l'utilisation d'un GPU sp√©cifique :

```bash
# Utiliser GPU 0
GPU_ID=0 USE_GPU=yes COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Utiliser GPU 1
GPU_ID=1 USE_GPU=yes COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
```

### Option 3: Docker Run Manuel

```bash
docker run -d \
    --name ebook2audio-worker-gpu0 \
    --gpus device=0 \
    -e REDIS_URL=redis://192.168.1.10:6379/0 \
    -e WORKER_ID=worker_gpu0 \
    -e CUDA_VISIBLE_DEVICES=0 \
    ebook2audiobook-worker:latest
```

### Multi-GPU sur Une Machine

```bash
# Worker 1 sur GPU 0
GPU_ID=0 WORKER_ID=w1_gpu0 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Worker 2 sur GPU 1
GPU_ID=1 WORKER_ID=w1_gpu1 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Worker 3 sur GPU 2
GPU_ID=2 WORKER_ID=w1_gpu2 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
```

---

## üíª D√©ploiement CPU

### Option 1: Auto-d√©tection

Si aucun GPU n'est d√©tect√©, utilise automatiquement le CPU :

```bash
# Sur machine sans GPU - d√©tection auto
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
# ‚Üí Utilise CPU automatiquement
```

### Option 2: CPU Forc√©

Forcer CPU m√™me si GPU disponible :

```bash
# Forcer CPU
USE_GPU=no COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
```

### Option 3: Docker Run Manuel

```bash
docker run -d \
    --name ebook2audio-worker-cpu \
    -e REDIS_URL=redis://192.168.1.10:6379/0 \
    -e WORKER_ID=worker_cpu \
    -e CUDA_VISIBLE_DEVICES="" \
    ebook2audiobook-worker:latest
```

**Important** : `CUDA_VISIBLE_DEVICES=""` force le mode CPU !

### Build Image CPU

```bash
# Build worker avec PyTorch CPU
TORCH_VERSION=cpu ./scripts/distributed/build-worker-image.sh
```

---

## üîÄ Cluster Mixte CPU + GPU

### Exemple : 2 GPU + 1 CPU

```bash
# Machine 1 - GPU (192.168.1.11)
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
# ‚Üí Auto: GPU d√©tect√© et utilis√©

# Machine 2 - GPU (192.168.1.12)
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
# ‚Üí Auto: GPU d√©tect√© et utilis√©

# Machine 3 - CPU seulement (192.168.1.13)
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
# ‚Üí Auto: Pas de GPU, utilise CPU

# Coordinator
NUM_WORKERS=3 ./scripts/distributed/start-coordinator.sh
```

**Flower Dashboard** montrera :
- Worker 1: device_type = 'cuda', gpu_memory_free = 11000 MB
- Worker 2: device_type = 'cuda', gpu_memory_free = 11000 MB
- Worker 3: device_type = 'cpu', gpu_memory_free = 0

### R√©partition des T√¢ches

Celery distribue les chapitres **√©quitablement** :
- Chapitre 0 ‚Üí Worker GPU 1 (rapide)
- Chapitre 1 ‚Üí Worker GPU 2 (rapide)
- Chapitre 2 ‚Üí Worker CPU (lent)
- Chapitre 3 ‚Üí Worker GPU 1 (rapide)
- ...

‚ö†Ô∏è **Attention** : Worker CPU sera **beaucoup plus lent** (10-50x selon mod√®le).

---

## ‚ö° Performances Compar√©es

### XTTS v2 (mod√®le par d√©faut)

| Device | Temps/Phrase (10 mots) | Speedup vs CPU |
|--------|------------------------|----------------|
| RTX 4090 | 0.5s | 40x |
| RTX 3090 | 0.8s | 25x |
| Tesla V100 | 1.2s | 17x |
| GTX 1080 Ti | 2s | 10x |
| **CPU (16 cores)** | **20s** | **1x** |

### Livre de 300 pages (~500 chapitres, ~10,000 phrases)

| Configuration | Temps Total |
|---------------|-------------|
| 1x RTX 4090 | 1.5 heures |
| 1x RTX 3090 | 2.2 heures |
| 1x CPU (16 cores) | **55 heures** |
| **2x RTX 4090** | **45 minutes** |
| **4x RTX 3090** | **33 minutes** |
| **Mixte: 2 GPU + 1 CPU** | **50 minutes** ‚ö†Ô∏è |

‚ö†Ô∏è Dans cluster mixte, les workers CPU ralentissent la progression globale !

---

## üîç Monitoring CPU vs GPU

### Health Check

Appeler le health check d'un worker :

```bash
# Via Flower dashboard
open http://192.168.1.10:5555

# Via Celery CLI
celery -A lib.distributed.celery_app inspect stats
```

**R√©ponse pour GPU Worker** :
```json
{
  "status": "ok",
  "device_type": "cuda",
  "gpu_available": true,
  "gpu_count": 1,
  "gpu_memory_free_mb": 11000,
  "tts_model_loaded": true,
  "cached_models": ["xtts_jenny_cuda"]
}
```

**R√©ponse pour CPU Worker** :
```json
{
  "status": "ok",
  "device_type": "cpu",
  "gpu_available": false,
  "gpu_count": 0,
  "gpu_memory_free_mb": 0,
  "tts_model_loaded": true,
  "cached_models": ["xtts_jenny_cpu"]
}
```

### Logs Worker

```bash
# GPU Worker
docker logs ebook2audio-worker-gpu0
# ‚Üí "Loading TTS model: xtts_jenny_cuda (device: cuda, GPU available: True)"

# CPU Worker
docker logs ebook2audio-worker-cpu
# ‚Üí "Loading TTS model: xtts_jenny_cpu (device: cpu, GPU available: False)"
```

---

## üõ†Ô∏è Troubleshooting

### GPU non d√©tect√©

**Sympt√¥mes** :
```
WARNING: GPU requested but not available, falling back to CPU
```

**Diagnostic** :
```bash
# V√©rifier NVIDIA driver
nvidia-smi

# V√©rifier Docker GPU support
docker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# V√©rifier PyTorch
docker exec ebook2audio-worker-gpu0 python -c "import torch; print(torch.cuda.is_available())"
```

**Solutions** :
1. Installer NVIDIA drivers
2. Installer nvidia-docker2
3. Rebuild image avec bon TORCH_VERSION

### Worker CPU trop lent

**Sympt√¥mes** :
- Conversion prend des heures
- CPU usage 100%

**Solutions** :
1. Ajouter plus de workers GPU
2. Retirer worker CPU du cluster
3. Utiliser mod√®le TTS plus l√©ger (Piper, edge-tts)

### OOM sur GPU

**Sympt√¥mes** :
```
RuntimeError: CUDA out of memory
```

**Solutions** :
```bash
# R√©duire batch size (si applicable)
# Ou utiliser GPU avec plus de VRAM

# V√©rifier m√©moire disponible
nvidia-smi

# Restart worker pour clear cache
docker restart ebook2audio-worker-gpu0
```

---

## üìã Checklist D√©ploiement

### Workers GPU

- [ ] NVIDIA drivers install√©s
- [ ] nvidia-docker2 install√©
- [ ] `nvidia-smi` fonctionne
- [ ] Image built avec TORCH_VERSION=cuda124
- [ ] Variable `USE_GPU=auto` ou `yes`
- [ ] Tester: `docker run --gpus all nvidia/cuda nvidia-smi`

### Workers CPU

- [ ] Image built avec TORCH_VERSION=cpu
- [ ] Variable `USE_GPU=no` ou auto (si pas GPU)
- [ ] Variable `CUDA_VISIBLE_DEVICES=""`
- [ ] Accepter performances r√©duites

---

## üí° Recommandations

### ‚úÖ Faire

- Utiliser GPU pour production
- Mixer GPU/CPU si n√©cessaire
- 1 worker par GPU pour isolation
- Monitorer via Flower dashboard
- CPU workers pour tests/dev uniquement

### ‚ùå √âviter

- N'utilisez pas CPU en production (trop lent)
- Ne partagez pas 1 GPU entre plusieurs workers (OOM)
- N'oubliez pas de configurer CUDA_VISIBLE_DEVICES

---

## üéØ Exemples Complets

### Cluster Production (GPU uniquement)

```bash
# Coordinator
./scripts/distributed/start-coordinator.sh

# Worker 1 - Machine avec 2x RTX 3090
GPU_ID=0 WORKER_ID=m1_gpu0 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
GPU_ID=1 WORKER_ID=m1_gpu1 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Worker 2 - Machine avec 1x RTX 4090
GPU_ID=0 WORKER_ID=m2_gpu0 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Total: 3 workers GPU ‚Üí 3x speedup
```

### Cluster Dev/Test (Mixte)

```bash
# Coordinator
./scripts/distributed/start-coordinator.sh

# Worker GPU pour performance
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Worker CPU pour test
USE_GPU=no WORKER_ID=test_cpu COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# Total: 1 GPU + 1 CPU pour tests
```

---

**Le syst√®me s'adapte automatiquement √† votre mat√©riel ! üöÄ**
