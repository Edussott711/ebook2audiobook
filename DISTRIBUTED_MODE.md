# Mode DistribuÃ© - Guide Complet

## ğŸ¯ Vue d'ensemble

Le mode distribuÃ© permet de **parallÃ©liser la conversion TTS** en utilisant plusieurs machines Ã©quipÃ©es de GPU, rÃ©duisant significativement le temps de traitement.

**Architecture** : Celery + Redis
**Stockage partagÃ©** : âŒ **Pas nÃ©cessaire !** Audio transfÃ©rÃ© via Redis (base64)

---

## ğŸš€ Quick Start (3 minutes)

### Option 1: Script interactif (RecommandÃ©)

```bash
# Setup automatique
./scripts/distributed/setup-cluster.sh

# Suivre les instructions pour configurer coordinator ou worker
```

### Option 2: Multi-machines avec Docker

```bash
# Machine 1 (Coordinator)
./scripts/distributed/start-coordinator.sh

# Machine 2+ (Workers - sur chaque machine avec GPU)
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# AccÃ©der Ã  l'interface web
open http://192.168.1.10:7860
```

### Option 3: Machine locale avec plusieurs GPUs

```bash
# 1. DÃ©marrer Redis
docker run -d -p 6379:6379 --name ebook2audio-redis redis:7-alpine

# 2. DÃ©marrer les workers (1 par GPU)
# Terminal 1 - Worker 1
WORKER_ID=worker_1 CUDA_VISIBLE_DEVICES=0 python app.py --worker_mode

# Terminal 2 - Worker 2
WORKER_ID=worker_2 CUDA_VISIBLE_DEVICES=1 python app.py --worker_mode

# 3. Lancer la conversion (coordinator)
python app.py --headless \
  --distributed \
  --num_workers 2 \
  --ebook input/book.epub \
  --language eng \
  --voice jenny
```

---

## ğŸ“Š Performance

### Gains attendus

| Configuration | Temps | Speedup |
|---------------|-------|---------|
| SÃ©quentiel (1 GPU) | 6h | 1x |
| **DistribuÃ© (2 workers)** | **3h** | **2x** |
| **DistribuÃ© (4 workers)** | **1.5h** | **4x** |
| **DistribuÃ© (8 workers)** | **45min** | **8x** |

**Scaling linÃ©aire** jusqu'Ã  10-20 workers !

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COORDINATOR  â”‚  1. Distribue les chapitres
â”‚  (Master)    â”‚     via Celery tasks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  2. ReÃ§oit audio base64
       â”‚          3. Combine & sauvegarde
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         REDIS                â”‚
â”‚  â€¢ Message Broker (Celery)   â”‚
â”‚  â€¢ Result Backend            â”‚
â”‚  â€¢ Audio Transfer (base64)   â”‚  â† Pas de NFS/S3 !
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼        â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”
â”‚Work1â”‚  â”‚Work2â”‚  â”‚Work3â”‚  â”‚WorkNâ”‚
â”‚GPU0 â”‚  â”‚GPU1 â”‚  â”‚GPU2 â”‚  â”‚GPUN â”‚  â† Machines indÃ©pendantes
â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜
  TTS      TTS      TTS      TTS
   â”‚        â”‚        â”‚        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       (Audio encodÃ© base64
        et retournÃ© via Redis)
```

**Avantages** :
- âœ… Aucun stockage partagÃ© nÃ©cessaire (NFS/S3/etc.)
- âœ… DÃ©ploiement simplifiÃ© avec `docker run` sur chaque machine
- âœ… Workers complÃ¨tement indÃ©pendants
- âœ… Audio transfÃ©rÃ© directement via Redis

---

## âš™ï¸ Configuration

### Variables d'environnement

```bash
# Redis (seule configuration nÃ©cessaire!)
export REDIS_URL=redis://localhost:6379/0

# Worker
export WORKER_ID=worker_1
export CUDA_VISIBLE_DEVICES=0
```

### Arguments CLI (Coordinator)

```bash
python app.py --headless \
  --distributed \                     # Active le mode distribuÃ©
  --num_workers 4 \                   # Nombre de workers
  --redis_url redis://redis:6379/0 \  # URL Redis
  --ebook book.epub
```

### Arguments CLI (Worker)

```bash
python app.py --worker_mode  # Lance en mode worker
# Utilise REDIS_URL, WORKER_ID et CUDA_VISIBLE_DEVICES des env vars
```

---

## ğŸ³ Docker DÃ©ploiement

### Option 1: Scripts automatiques (RecommandÃ©)

```bash
# Sur la machine coordinator
./scripts/distributed/start-coordinator.sh

# Sur chaque machine worker
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
```

Voir [scripts/distributed/README.md](scripts/distributed/README.md) pour plus de dÃ©tails.

### Option 2: Docker Compose (machine locale uniquement)

```yaml
# docker-compose.distributed.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 2gb

  coordinator:
    build: .
    environment:
      - REDIS_URL=redis://redis:6379/0
      - NUM_WORKERS=2
    volumes:
      - ./input:/app/input
      - ./output:/app/output
    depends_on:
      - redis

  worker:
    build:
      dockerfile: Dockerfile.worker
    environment:
      - REDIS_URL=redis://redis:6379/0
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      replicas: 2
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

**Usage** :
```bash
docker-compose -f docker-compose.distributed.yml up -d
```

---

## ğŸ“š Documentation dÃ©taillÃ©e

### Planning et architecture
- [DISTRIBUTED_MODE_PLAN.md](DISTRIBUTED_MODE_PLAN.md) - Plan complet d'implÃ©mentation
- [ARCHITECTURE_DIAGRAM.md](ARCHITECTURE_DIAGRAM.md) - Diagrammes visuels
- [TECHNICAL_SPECIFICATIONS.md](TECHNICAL_SPECIFICATIONS.md) - SpÃ©cifications techniques

### Guides
- [IMPLEMENTATION_GUIDE.md](IMPLEMENTATION_GUIDE.md) - Guide d'implÃ©mentation (8 semaines)
- [README-DISTRIBUTED.md](README-DISTRIBUTED.md) - Guide utilisateur complet

### Index
- [DISTRIBUTED_MODE_INDEX.md](DISTRIBUTED_MODE_INDEX.md) - Navigation dans la documentation

---

## ğŸ”§ Troubleshooting

### Workers ne dÃ©marrent pas

**SymptÃ´me** : `celery worker` ne dÃ©marre pas

**Solutions** :
1. VÃ©rifier que Redis est accessible : `redis-cli ping`
2. VÃ©rifier les logs : `celery -A lib.distributed.celery_app inspect active`
3. VÃ©rifier les dÃ©pendances : `pip install -r requirements-distributed.txt`

### Conversion bloquÃ©e

**SymptÃ´me** : Aucune progression visible

**Diagnostic** :
```bash
# Flower monitoring
docker-compose up -d flower
open http://localhost:5555

# VÃ©rifier queue Redis
redis-cli LLEN tts_queue
```

### GPU out of memory

**Solution** : 1 worker par GPU
```bash
# Chaque worker doit avoir son propre GPU
export CUDA_VISIBLE_DEVICES=0  # Worker 1
export CUDA_VISIBLE_DEVICES=1  # Worker 2
```

---

## ğŸ“ˆ Monitoring

### Flower Dashboard

```bash
# DÃ©marrer Flower
docker-compose up -d flower

# AccÃ©der au dashboard
open http://localhost:5555
```

**Features** :
- TÃ¢ches en temps rÃ©el
- Statistiques par worker
- Retry history
- Logs centralisÃ©s

---

## ğŸ” Production

### Checklist sÃ©curitÃ©

- [ ] Redis avec password : `REDIS_URL=redis://:password@host:6379/0`
- [ ] Redis avec TLS : `rediss://host:6379/0`
- [ ] Flower avec authentification
- [ ] Firewall pour limiter accÃ¨s Redis
- [ ] S3 avec IAM roles (pas de credentials en clair)

---

## â“ FAQ

**Q: Ai-je besoin d'un stockage partagÃ© (NFS/S3) ?**
R: âŒ **Non !** Les fichiers audio sont transfÃ©rÃ©s directement via Redis (base64). Chaque machine est complÃ¨tement indÃ©pendante.

**Q: Combien de workers puis-je avoir ?**
R: Autant que de GPUs. En pratique, 2-20 workers est optimal.

**Q: Puis-je mixer GPUs et CPUs ?**
R: Oui, mais les workers CPU seront beaucoup plus lents.

**Q: Quelle consommation rÃ©seau ?**
R: ~1-10MB par chapitre (audio MP3 encodÃ© base64 via Redis). Un livre de 50 chapitres = ~250MB transfÃ©rÃ©s. NÃ©gligeable sur rÃ©seau local gigabit.

**Q: Redis peut-il gÃ©rer de gros fichiers audio ?**
R: Oui ! Redis 7 gÃ¨re facilement des valeurs de 10-20MB. Configurez `maxmemory` selon vos besoins (voir docker-compose).

**Q: Puis-je reprendre aprÃ¨s interruption ?**
R: Oui ! Le systÃ¨me de checkpoint distribuÃ© (stockÃ© dans Redis) permet le resume.

**Q: Comment dÃ©ployer sur plusieurs machines ?**
R: Utilisez `./scripts/distributed/setup-cluster.sh` ou suivez [scripts/distributed/README.md](scripts/distributed/README.md).

---

## ğŸ‰ Exemple complet

### ScÃ©nario : 1 Coordinator + 3 Workers sur 4 machines

```bash
# === MACHINE 1 (Coordinator - 192.168.1.10) ===
./scripts/distributed/start-coordinator.sh
# Coordinator dÃ©marrÃ© sur http://192.168.1.10:7860
# Flower dashboard sur http://192.168.1.10:5555

# === MACHINE 2 (Worker 1 - GPU Tesla V100) ===
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# === MACHINE 3 (Worker 2 - GPU RTX 3090) ===
COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# === MACHINE 4 (Worker 3 - 2x RTX 4090) ===
# Lancer 2 workers (1 par GPU)
GPU_ID=0 WORKER_ID=worker_m4_gpu0 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh
GPU_ID=1 WORKER_ID=worker_m4_gpu1 COORDINATOR_IP=192.168.1.10 ./scripts/distributed/start-worker.sh

# === Retour sur COORDINATOR ===
# VÃ©rifier les workers dans Flower
open http://192.168.1.10:5555
# Vous devriez voir 4 workers actifs

# Lancer conversion via interface web
open http://192.168.1.10:7860
# Ou en CLI :
docker exec ebook2audio-coordinator python app.py \
  --headless \
  --distributed \
  --num_workers 4 \
  --ebook /app/input/harry_potter.epub \
  --language eng

# RÃ©sultat : 4x plus rapide qu'en sÃ©quentiel! ğŸš€
```

---

## ğŸ“ Support

En cas de problÃ¨me :
1. Consulter [README-DISTRIBUTED.md](README-DISTRIBUTED.md#troubleshooting)
2. VÃ©rifier les logs : `docker logs ebook2audio-coordinator`
3. Ouvrir une issue : [GitHub Issues](https://github.com/yourusername/ebook2audiobook/issues)

---

**Bon audiobook distribuÃ© ! ğŸµâš¡**

**CrÃ©Ã© le** : 2025-11-07
**Version** : 1.0
