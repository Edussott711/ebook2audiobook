# Mode DistribuÃ© - Guide Complet

## ğŸ¯ Vue d'ensemble

Le mode distribuÃ© permet de **parallÃ©liser la conversion TTS** en utilisant plusieurs machines Ã©quipÃ©es de GPU, rÃ©duisant significativement le temps de traitement.

**Architecture** : Celery + Redis

---

## ğŸš€ Quick Start (3 minutes)

### 1. DÃ©marrer le cluster

```bash
# 1. DÃ©marrer Redis
docker run -d -p 6379:6379 --name ebook2audio-redis redis:7-alpine

# 2. DÃ©marrer les workers (1 par GPU)
# Terminal 1 - Worker 1
export WORKER_ID=worker_1
export CUDA_VISIBLE_DEVICES=0
python app.py --worker-mode

# Terminal 2 - Worker 2
export WORKER_ID=worker_2
export CUDA_VISIBLE_DEVICES=1
python app.py --worker-mode

# 3. Lancer la conversion (coordinator)
python app.py --headless \
  --distributed \
  --num-workers 2 \
  --ebook input/book.epub \
  --language en \
  --voice jenny
```

### 2. Avec Docker Compose (RecommandÃ©)

```bash
# DÃ©marrer tout le cluster
./scripts/start-distributed.sh

# Ou manuellement
docker-compose -f docker-compose.distributed.yml up -d --scale worker=3

# Lancer conversion
docker exec ebook2audio-coordinator python app.py \
  --headless --distributed --num-workers 3 \
  --ebook /app/input/book.epub
```

---

## ğŸ“Š Performance

### Gains attendus

| Configuration | Temps | Speedup |
|---------------|-------|---------|
| SÃ©quentiel (1 GPU) | 6h | 1x |
| **DistribuÃ© (2 workers)** | **3h** | **2x** |
| **DistribuÃ© (4 workers)** | **1.5h** | **4x** |
| **DistribuÃ© (8 workers)** | **45min** | **8x** |

**Scaling linÃ©aire** jusqu'Ã  10-20 workers !

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COORDINATOR  â”‚  Distribue les chapitres
â”‚  (Master)    â”‚  via Celery tasks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  Message Broker
â”‚  Queue + KV  â”‚  + Result Backend
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼        â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”
â”‚Work1â”‚  â”‚Work2â”‚  â”‚Work3â”‚  â”‚WorkNâ”‚
â”‚GPU0 â”‚  â”‚GPU1 â”‚  â”‚GPU2 â”‚  â”‚GPUN â”‚
â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜
   â”‚        â”‚        â”‚        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Shared       â”‚
      â”‚ Storage      â”‚
      â”‚ (NFS/S3)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### Variables d'environnement

```bash
# Redis
export REDIS_URL=redis://localhost:6379/0

# Stockage partagÃ©
export SHARED_STORAGE_TYPE=nfs  # ou s3, local
export SHARED_STORAGE_PATH=/mnt/shared

# Worker
export WORKER_ID=worker_1
export CUDA_VISIBLE_DEVICES=0
```

### Arguments CLI

```bash
python app.py --headless \
  --distributed \                    # Active le mode distribuÃ©
  --num-workers 4 \                  # Nombre de workers
  --redis-url redis://redis:6379/0 \ # URL Redis
  --storage-type nfs \               # Type de stockage
  --storage-path /mnt/shared \       # Chemin stockage
  --ebook book.epub
```

---

## ğŸ³ Docker Compose

### Configuration minimale

```yaml
# docker-compose.distributed.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  coordinator:
    build: .
    environment:
      - NUM_WORKERS=2
    volumes:
      - ./input:/app/input
      - ./output:/app/output
    depends_on:
      - redis
    command: python app.py --headless --distributed ...

  worker:
    build: .
    environment:
      - REDIS_URL=redis://redis:6379/0
    deploy:
      replicas: 2  # Nombre de workers
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - redis
    command: python app.py --worker-mode
```

---

## ğŸ“š Documentation dÃ©taillÃ©e

### Planning et architecture
- [DISTRIBUTED_MODE_PLAN.md](DISTRIBUTED_MODE_PLAN.md) - Plan complet d'implÃ©mentation
- [ARCHITECTURE_DIAGRAM.md](ARCHITECTURE_DIAGRAM.md) - Diagrammes visuels
- [TECHNICAL_SPECIFICATIONS.md](TECHNICAL_SPECIFICATIONS.md) - SpÃ©cifications techniques

### Guides
- [IMPLEMENTATION_GUIDE.md](IMPLEMENTATION_GUIDE.md) - Guide d'implÃ©mentation (8 semaines)
- [README-DISTRIBUTED.md](README-DISTRIBUTED.md) - Guide utilisateur complet

### Index
- [DISTRIBUTED_MODE_INDEX.md](DISTRIBUTED_MODE_INDEX.md) - Navigation dans la documentation

---

## ğŸ”§ Troubleshooting

### Workers ne dÃ©marrent pas

**SymptÃ´me** : `celery worker` ne dÃ©marre pas

**Solutions** :
1. VÃ©rifier que Redis est accessible : `redis-cli ping`
2. VÃ©rifier les logs : `celery -A lib.distributed.celery_app inspect active`
3. VÃ©rifier les dÃ©pendances : `pip install -r requirements-distributed.txt`

### Conversion bloquÃ©e

**SymptÃ´me** : Aucune progression visible

**Diagnostic** :
```bash
# Flower monitoring
docker-compose up -d flower
open http://localhost:5555

# VÃ©rifier queue Redis
redis-cli LLEN tts_queue
```

### GPU out of memory

**Solution** : 1 worker par GPU
```bash
# Chaque worker doit avoir son propre GPU
export CUDA_VISIBLE_DEVICES=0  # Worker 1
export CUDA_VISIBLE_DEVICES=1  # Worker 2
```

---

## ğŸ“ˆ Monitoring

### Flower Dashboard

```bash
# DÃ©marrer Flower
docker-compose up -d flower

# AccÃ©der au dashboard
open http://localhost:5555
```

**Features** :
- TÃ¢ches en temps rÃ©el
- Statistiques par worker
- Retry history
- Logs centralisÃ©s

---

## ğŸ” Production

### Checklist sÃ©curitÃ©

- [ ] Redis avec password : `REDIS_URL=redis://:password@host:6379/0`
- [ ] Redis avec TLS : `rediss://host:6379/0`
- [ ] Flower avec authentification
- [ ] Firewall pour limiter accÃ¨s Redis
- [ ] S3 avec IAM roles (pas de credentials en clair)

---

## â“ FAQ

**Q: Combien de workers puis-je avoir ?**
R: Autant que de GPUs. En pratique, 2-20 workers est optimal.

**Q: Puis-je mixer GPUs et CPUs ?**
R: Oui, mais les workers CPU seront beaucoup plus lents.

**Q: Quelle consommation rÃ©seau ?**
R: ~1-5MB par chapitre (transfert audio). NÃ©gligeable sur rÃ©seau local.

**Q: Puis-je reprendre aprÃ¨s interruption ?**
R: Oui ! Le systÃ¨me de checkpoint distribuÃ© permet le resume.

---

## ğŸ‰ Exemple complet

```bash
# 1. DÃ©marrer le cluster (3 workers)
docker-compose -f docker-compose.distributed.yml up -d --scale worker=3

# 2. VÃ©rifier les workers
docker-compose -f docker-compose.distributed.yml ps

# 3. Lancer conversion
docker exec ebook2audio-coordinator python app.py \
  --headless \
  --distributed \
  --num-workers 3 \
  --ebook /app/input/harry_potter.epub \
  --language en \
  --voice jenny

# 4. Suivre dans Flower
open http://localhost:5555

# 5. RÃ©sultat dans output/
ls output/
# harry_potter.mp3  (3x plus rapide qu'en sÃ©quentiel!)
```

---

## ğŸ“ Support

En cas de problÃ¨me :
1. Consulter [README-DISTRIBUTED.md](README-DISTRIBUTED.md#troubleshooting)
2. VÃ©rifier les logs : `docker logs ebook2audio-coordinator`
3. Ouvrir une issue : [GitHub Issues](https://github.com/yourusername/ebook2audiobook/issues)

---

**Bon audiobook distribuÃ© ! ğŸµâš¡**

**CrÃ©Ã© le** : 2025-11-07
**Version** : 1.0
