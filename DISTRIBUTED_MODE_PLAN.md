# Plan d'impl√©mentation : Mode de Parall√©lisme Distribu√©

## üìã Vue d'ensemble

Ajout d'un mode de traitement distribu√© multi-machines pour parall√©liser la conversion TTS de livres audio √† grande √©chelle.

### Probl√®me actuel
- **Bottleneck TTS** : Les phrases sont trait√©es s√©quentiellement (lib/functions.py:1498-1508)
- **Sous-utilisation GPU/CPU** : Un seul worker par instance
- **Scalabilit√© limit√©e** : Impossible d'utiliser plusieurs machines pour acc√©l√©rer

### Objectif
Permettre la distribution du travail sur N machines pour r√©duire le temps de traitement de plusieurs heures √† quelques minutes.

---

## üèóÔ∏è Architecture propos√©e

### Option 1 : Celery + Redis (RECOMMAND√â)
**Avantages** :
- ‚úÖ Framework mature et battle-tested
- ‚úÖ Retry automatique et gestion des pannes
- ‚úÖ Monitoring int√©gr√© (Flower)
- ‚úÖ Scaling horizontal facile
- ‚úÖ Compatible avec l'√©cosyst√®me Python existant

**Inconv√©nients** :
- ‚ùå D√©pendance suppl√©mentaire (Redis/RabbitMQ)
- ‚ùå Overhead pour petits jobs

**Stack technique** :
```
Master Node:
‚îú‚îÄ‚îÄ Celery Beat (optionnel - scheduling)
‚îú‚îÄ‚îÄ Redis (message broker + result backend)
‚îî‚îÄ‚îÄ Coordinator service (distribue les chapitres)

Worker Nodes (1-N):
‚îú‚îÄ‚îÄ Celery Worker
‚îú‚îÄ‚îÄ TTS Engine (XTTS charg√© en m√©moire)
‚îî‚îÄ‚îÄ Shared storage mount (NFS/S3)
```

### Option 2 : Ray Distributed
**Avantages** :
- ‚úÖ Optimis√© pour ML/AI workloads
- ‚úÖ Gestion automatique des ressources GPU
- ‚úÖ API Python native et simple
- ‚úÖ Dashboard de monitoring int√©gr√©

**Inconv√©nients** :
- ‚ùå Overhead m√©moire plus √©lev√©
- ‚ùå Moins mature pour production

### Option 3 : Architecture custom avec ZeroMQ
**Avantages** :
- ‚úÖ Contr√¥le total sur le syst√®me
- ‚úÖ L√©g√®ret√© maximale
- ‚úÖ Pas de d√©pendances lourdes

**Inconv√©nients** :
- ‚ùå D√©veloppement complet de la logique de distribution
- ‚ùå Pas de retry/monitoring int√©gr√©
- ‚ùå Maintenance importante

---

## üéØ D√©cision : Celery + Redis

### Justification
1. **Maturit√©** : Production-ready avec millions d'utilisateurs
2. **Int√©gration facile** : Compatible avec l'architecture actuelle
3. **Monitoring** : Flower pour visualiser les t√¢ches en temps r√©el
4. **Fault tolerance** : Retry automatique + dead letter queues

---

## üì¶ Composants √† d√©velopper

### 1. Distributed Coordinator (`lib/distributed/coordinator.py`)
**Responsabilit√©s** :
- D√©couper le livre en unit√©s de travail (chapitres ou phrases)
- Envoyer les t√¢ches √† la queue Celery
- Agr√©ger les r√©sultats
- Mettre √† jour les checkpoints distribu√©s

**API** :
```python
class DistributedCoordinator:
    def __init__(self, session_id, num_workers):
        self.session_id = session_id
        self.redis_client = Redis(...)
        self.checkpoint_manager = DistributedCheckpointManager()

    def distribute_book(self, chapters: List[Chapter]):
        """Distribue les chapitres aux workers"""
        tasks = []
        for chapter in chapters:
            task = process_chapter.delay(
                chapter_id=chapter.id,
                sentences=chapter.sentences,
                session_id=self.session_id
            )
            tasks.append(task)
        return tasks

    def wait_and_aggregate(self, tasks):
        """Attend la fin et agr√®ge les r√©sultats"""
        results = [task.get() for task in tasks]
        return self.combine_audio_files(results)
```

### 2. Celery Tasks (`lib/distributed/tasks.py`)
**T√¢ches d√©finies** :
```python
@celery_app.task(bind=True, max_retries=3)
def process_chapter(self, chapter_id, sentences, session_id, tts_config):
    """Traite un chapitre complet sur un worker"""
    try:
        # Charger le mod√®le TTS (mis en cache)
        tts_engine = get_cached_tts_engine(tts_config)

        # Traiter chaque phrase
        audio_files = []
        for sentence in sentences:
            audio_path = tts_engine.convert_sentence2audio(sentence)
            audio_files.append(audio_path)

        # Combiner les audios du chapitre
        combined_path = combine_chapter_audio(audio_files)

        # Uploader vers stockage partag√©
        shared_path = upload_to_shared_storage(combined_path)

        # Mettre √† jour checkpoint
        update_distributed_checkpoint(session_id, chapter_id, 'completed')

        return {
            'chapter_id': chapter_id,
            'audio_path': shared_path,
            'duration': get_audio_duration(shared_path)
        }
    except Exception as exc:
        # Retry avec backoff exponentiel
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)

@celery_app.task
def process_sentence(sentence_id, text, session_id, tts_config):
    """Traite une phrase unique (granularit√© fine)"""
    # Impl√©mentation similaire mais niveau phrase
    pass
```

### 3. Distributed Checkpoint Manager (`lib/distributed/checkpoint_manager.py`)
**Extensions au CheckpointManager actuel** :
```python
class DistributedCheckpointManager(CheckpointManager):
    def __init__(self, session_id, redis_client):
        super().__init__(session_id)
        self.redis = redis_client
        self.lock_key = f"checkpoint_lock:{session_id}"

    def save_checkpoint(self, stage, data):
        """Sauvegarde atomique avec Redis lock"""
        with redis_lock(self.redis, self.lock_key):
            # Fusion des donn√©es de tous les workers
            existing = self.redis.hgetall(f"checkpoint:{self.session_id}")
            merged = {**existing, **data}

            # Sauvegarde locale + Redis
            super().save_checkpoint(stage, merged)
            self.redis.hset(f"checkpoint:{self.session_id}", mapping=merged)

    def get_pending_chapters(self):
        """Retourne les chapitres non trait√©s pour resume"""
        checkpoint = self.load_checkpoint()
        all_chapters = checkpoint.get('total_chapters', [])
        completed = checkpoint.get('converted_chapters', [])
        return [ch for ch in all_chapters if ch not in completed]
```

### 4. Worker Service (`lib/distributed/worker.py`)
**Responsabilit√©s** :
- D√©marrer un worker Celery
- Pr√©-charger le mod√®le TTS en m√©moire GPU
- Traiter les t√¢ches de la queue
- Reporter l'√©tat au coordinator

**D√©marrage** :
```python
class DistributedWorker:
    def __init__(self, worker_id, gpu_id=None):
        self.worker_id = worker_id
        self.gpu_id = gpu_id
        self.tts_engine = None

    def start(self):
        """Lance le worker Celery"""
        # Pr√©-charge le mod√®le TTS
        self.tts_engine = initialize_tts_engine(gpu_id=self.gpu_id)

        # Cache global pour √©viter recharges
        set_global_tts_engine(self.tts_engine)

        # D√©marre Celery worker
        celery_app.worker_main([
            'worker',
            f'--hostname=worker_{self.worker_id}@%h',
            '--loglevel=info',
            '--concurrency=1',  # 1 t√¢che/worker pour GPU isolation
            f'--queues=tts_queue',
        ])
```

### 5. Shared Storage Handler (`lib/distributed/storage.py`)
**Options de stockage** :
```python
class SharedStorageHandler:
    def __init__(self, storage_type='nfs'):
        # storage_type: 'nfs', 's3', 'local' (pour testing)
        self.storage_type = storage_type
        self.base_path = self._get_base_path()

    def upload_audio(self, local_path, session_id, chapter_id):
        """Upload vers stockage partag√©"""
        if self.storage_type == 'nfs':
            # Simple copy vers mount NFS
            shared_path = f"{self.base_path}/{session_id}/{chapter_id}.mp3"
            shutil.copy2(local_path, shared_path)
        elif self.storage_type == 's3':
            # Upload vers S3
            s3_client.upload_file(local_path, bucket, key)
        return shared_path

    def download_audio(self, shared_path, local_path):
        """R√©cup√®re depuis stockage partag√©"""
        # Impl√©mentation inverse
        pass
```

---

## üîß Modifications du code existant

### 1. `lib/functions.py` - convert_ebook()
**Changements** :
```python
def convert_ebook(ebook_file, voice_name, language, ..., distributed_mode=False, num_workers=1):
    # ... code existant ...

    if distributed_mode:
        from lib.distributed.coordinator import DistributedCoordinator

        coordinator = DistributedCoordinator(
            session_id=session_id,
            num_workers=num_workers,
            redis_url=os.getenv('REDIS_URL', 'redis://localhost:6379')
        )

        # Distribution des chapitres
        tasks = coordinator.distribute_book(chapters)

        # Attente et agr√©gation
        audio_files = coordinator.wait_and_aggregate(tasks)

    else:
        # Mode s√©quentiel actuel (pas de changement)
        audio_files = convert_chapters2audio(...)

    # ... suite du code ...
```

### 2. `app.py` - Nouveaux arguments CLI
```python
parser.add_argument(
    '--distributed',
    action='store_true',
    help='Enable distributed processing mode'
)
parser.add_argument(
    '--num-workers',
    type=int,
    default=1,
    help='Number of distributed workers to use'
)
parser.add_argument(
    '--redis-url',
    type=str,
    default='redis://localhost:6379',
    help='Redis URL for distributed coordination'
)
parser.add_argument(
    '--worker-mode',
    action='store_true',
    help='Start as a worker node (not coordinator)'
)
```

### 3. Nouvelle configuration (`lib/conf.py`)
```python
# Distributed mode settings
DISTRIBUTED_MODE_ENABLED = os.getenv('DISTRIBUTED_MODE', 'false').lower() == 'true'
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')
CELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', REDIS_URL)
CELERY_RESULT_BACKEND = os.getenv('CELERY_RESULT_BACKEND', REDIS_URL)
SHARED_STORAGE_TYPE = os.getenv('SHARED_STORAGE_TYPE', 'nfs')  # nfs, s3, local
SHARED_STORAGE_PATH = os.getenv('SHARED_STORAGE_PATH', '/mnt/shared')
```

---

## üê≥ D√©ploiement Docker

### 1. Docker Compose pour cluster distribu√©
**`docker-compose.distributed.yml`** :
```yaml
version: '3.8'

services:
  # Redis broker
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # Coordinator (master)
  coordinator:
    build: .
    environment:
      - DISTRIBUTED_MODE=true
      - REDIS_URL=redis://redis:6379
      - SHARED_STORAGE_PATH=/mnt/shared
    volumes:
      - ./input:/app/input
      - ./output:/app/output
      - shared_audio:/mnt/shared
    depends_on:
      - redis
    command: python app.py --distributed --num-workers 3 --script_mode headless

  # Workers (scalable)
  worker:
    build: .
    environment:
      - DISTRIBUTED_MODE=true
      - REDIS_URL=redis://redis:6379
      - SHARED_STORAGE_PATH=/mnt/shared
    volumes:
      - shared_audio:/mnt/shared
    depends_on:
      - redis
    deploy:
      replicas: 3  # Nombre de workers
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: python app.py --worker-mode

  # Monitoring Flower
  flower:
    image: mher/flower
    environment:
      - CELERY_BROKER_URL=redis://redis:6379
    ports:
      - "5555:5555"
    depends_on:
      - redis

volumes:
  redis_data:
  shared_audio:
```

### 2. Dockerfile multi-stage pour workers
```dockerfile
# Utiliser le Dockerfile existant comme base
FROM <base_image> as worker

# Installer Celery et d√©pendances
RUN pip install celery[redis] flower

# Copier le code de distribution
COPY lib/distributed /app/lib/distributed

# Point d'entr√©e configur√© pour worker/coordinator
COPY entrypoint-distributed.sh /app/
RUN chmod +x /app/entrypoint-distributed.sh
ENTRYPOINT ["/app/entrypoint-distributed.sh"]
```

**`entrypoint-distributed.sh`** :
```bash
#!/bin/bash
if [ "$WORKER_MODE" = "true" ]; then
    echo "Starting worker node..."
    celery -A lib.distributed.celery_app worker --loglevel=info
else
    echo "Starting coordinator node..."
    exec python app.py "$@"
fi
```

---

## üìä Strat√©gie de distribution

### Granularit√© : Chapitre vs Phrase

#### Option A : Distribution par chapitre (RECOMMAND√â)
**Avantages** :
- ‚úÖ Moins d'overhead de communication
- ‚úÖ Checkpoints naturels (par chapitre)
- ‚úÖ Load balancing acceptable (livres = 20-50 chapitres)

**Inconv√©nients** :
- ‚ùå D√©s√©quilibre si chapitres de tailles variables

**Impl√©mentation** :
```python
# 1 t√¢che = 1 chapitre complet
for chapter in chapters:
    process_chapter.delay(chapter)
```

#### Option B : Distribution par phrase
**Avantages** :
- ‚úÖ Load balancing optimal
- ‚úÖ Granularit√© maximale

**Inconv√©nients** :
- ‚ùå Overhead √©lev√© (milliers de t√¢ches)
- ‚ùå Gestion complexe des checkpoints

**Impl√©mentation** :
```python
# 1 t√¢che = 1 phrase
for sentence in all_sentences:
    process_sentence.delay(sentence)
```

#### Option C : Hybride (Best of both)
**Distribution par chapitre + parall√©lisme local des phrases** :
```python
@celery_app.task
def process_chapter(chapter):
    # Sur le worker, utiliser multiprocessing local pour les phrases
    with ProcessPoolExecutor(max_workers=4) as executor:
        audio_files = executor.map(process_sentence_local, chapter.sentences)
    return combine_chapter(audio_files)
```

**Recommandation** : Commencer avec **Option A** (par chapitre) pour simplicit√©.

---

## üîê Gestion des pannes

### 1. Retry automatique
```python
@celery_app.task(
    bind=True,
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 5},
    retry_backoff=True,
    retry_backoff_max=300,
    retry_jitter=True
)
def process_chapter(self, ...):
    # Celery g√®re automatiquement les retries
    pass
```

### 2. Dead Letter Queue
```python
# Configuration Celery
celery_app.conf.task_reject_on_worker_lost = True
celery_app.conf.task_acks_late = True  # ACK apr√®s succ√®s seulement
```

### 3. Health checks
```python
# Monitoring p√©riodique des workers
@celery_app.task
def health_check():
    return {'status': 'ok', 'gpu_available': torch.cuda.is_available()}

# Ping depuis coordinator
for worker in active_workers:
    health_check.apply_async(queue=worker.queue)
```

### 4. Graceful shutdown
```python
# Capture SIGTERM pour sauvegarde checkpoint
import signal

def save_checkpoint_and_exit(signum, frame):
    checkpoint_manager.save_checkpoint('interrupted', current_state)
    sys.exit(0)

signal.signal(signal.SIGTERM, save_checkpoint_and_exit)
```

---

## üìà Monitoring et observabilit√©

### 1. Flower Dashboard
- **URL** : http://localhost:5555
- **Fonctionnalit√©s** :
  - Visualisation des t√¢ches en temps r√©el
  - Statistiques par worker
  - Retry history
  - Logs centralis√©s

### 2. M√©triques custom
```python
from celery.signals import task_success, task_failure

@task_success.connect
def task_success_handler(sender=None, result=None, **kwargs):
    # Log vers Prometheus/Grafana
    metrics.increment('tts.chapters.completed')
    metrics.timing('tts.chapter.duration', result['duration'])

@task_failure.connect
def task_failure_handler(sender=None, exception=None, **kwargs):
    metrics.increment('tts.chapters.failed')
    logger.error(f"Task failed: {exception}")
```

### 3. Progress tracking
```python
# Update Redis avec progression en temps r√©el
def update_progress(session_id, completed_chapters, total_chapters):
    progress = (completed_chapters / total_chapters) * 100
    redis_client.set(f"progress:{session_id}", progress)
    redis_client.publish(f"progress_updates", json.dumps({
        'session_id': session_id,
        'progress': progress
    }))
```

---

## üß™ Tests

### 1. Tests unitaires
```python
# tests/test_distributed_coordinator.py
def test_distribute_chapters():
    coordinator = DistributedCoordinator(session_id='test', num_workers=2)
    chapters = [Chapter(id=1, sentences=['Hello']), Chapter(id=2, sentences=['World'])]

    tasks = coordinator.distribute_book(chapters)
    assert len(tasks) == 2

def test_checkpoint_sync():
    manager = DistributedCheckpointManager('test', redis_client)
    manager.save_checkpoint('audio_conversion_in_progress', {'chapter': 1})

    loaded = manager.load_checkpoint()
    assert loaded['chapter'] == 1
```

### 2. Tests d'int√©gration
```python
# tests/test_distributed_integration.py
@pytest.mark.integration
def test_full_distributed_conversion():
    # D√©marre cluster test (Redis + 2 workers)
    with DistributedTestCluster(num_workers=2):
        result = convert_ebook(
            'test.epub',
            distributed_mode=True,
            num_workers=2
        )
        assert os.path.exists(result['output_file'])
```

### 3. Tests de charge
```python
# Simuler 10 conversions simultan√©es
@pytest.mark.stress
def test_concurrent_books():
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [
            executor.submit(convert_ebook, f'book_{i}.epub', distributed_mode=True)
            for i in range(10)
        ]
        results = [f.result() for f in futures]
    assert all(r['status'] == 'completed' for r in results)
```

---

## üìö Documentation utilisateur

### D√©marrage rapide

#### Mode local (testing)
```bash
# D√©marrer Redis
docker run -d -p 6379:6379 redis:alpine

# Terminal 1 : D√©marrer workers
python app.py --worker-mode --num-workers 2

# Terminal 2 : Lancer conversion
python app.py --distributed --num-workers 2 --script_mode headless \
  --ebook input/book.epub --voice en_US/female
```

#### Mode cluster Docker
```bash
# Lancer cluster complet (coordinator + 3 workers + Redis)
docker-compose -f docker-compose.distributed.yml up --scale worker=3

# Monitoring
open http://localhost:5555  # Flower dashboard
```

### Configuration avanc√©e

#### Variables d'environnement
```bash
# Coordination
export DISTRIBUTED_MODE=true
export REDIS_URL=redis://my-redis-server:6379
export NUM_WORKERS=5

# Stockage partag√©
export SHARED_STORAGE_TYPE=s3
export AWS_S3_BUCKET=my-audiobooks-bucket

# Tuning
export CELERY_WORKER_CONCURRENCY=1  # T√¢ches simultan√©es/worker
export CELERY_TASK_TIME_LIMIT=3600  # Timeout 1h par chapitre
```

---

## üöÄ Feuille de route d'impl√©mentation

### Phase 1 : Infrastructure de base (Semaine 1-2)
1. ‚úÖ Installation et configuration Celery + Redis
2. ‚úÖ Cr√©ation de `lib/distributed/coordinator.py`
3. ‚úÖ Cr√©ation de `lib/distributed/tasks.py` avec task basique
4. ‚úÖ Tests unitaires pour coordinator

### Phase 2 : Int√©gration TTS (Semaine 3)
1. ‚úÖ Adaptation de `convert_chapters2audio()` pour mode distribu√©
2. ‚úÖ Impl√©mentation de `process_chapter` task complet
3. ‚úÖ Gestion du cache TTS sur workers
4. ‚úÖ Tests d'int√©gration bout-en-bout

### Phase 3 : Checkpoint distribu√© (Semaine 4)
1. ‚úÖ Extension de `CheckpointManager` ‚Üí `DistributedCheckpointManager`
2. ‚úÖ Synchronisation Redis des √©tats
3. ‚úÖ Tests de resume apr√®s panne
4. ‚úÖ Validation de la coh√©rence des donn√©es

### Phase 4 : Stockage partag√© (Semaine 5)
1. ‚úÖ Impl√©mentation `SharedStorageHandler` (NFS + S3)
2. ‚úÖ Int√©gration avec workers
3. ‚úÖ Tests de performance stockage
4. ‚úÖ Fallback sur stockage local si partag√© indisponible

### Phase 5 : Docker et d√©ploiement (Semaine 6)
1. ‚úÖ Cr√©ation `docker-compose.distributed.yml`
2. ‚úÖ Script `entrypoint-distributed.sh`
3. ‚úÖ Configuration multi-GPU
4. ‚úÖ Documentation d√©ploiement

### Phase 6 : Monitoring (Semaine 7)
1. ‚úÖ Int√©gration Flower
2. ‚úÖ M√©triques custom Prometheus
3. ‚úÖ Dashboard Grafana pour visualisation
4. ‚úÖ Alertes sur pannes workers

### Phase 7 : Optimisations (Semaine 8)
1. ‚úÖ Load balancing dynamique
2. ‚úÖ Compression des r√©sultats interm√©diaires
3. ‚úÖ Auto-scaling workers selon charge
4. ‚úÖ Benchmarks performance

---

## üìä M√©triques de succ√®s

| M√©trique | Cible |
|----------|-------|
| **Temps de conversion** | 5-10x plus rapide avec 5 workers |
| **Utilisation GPU** | >80% (vs <30% actuel) |
| **Taux de pannes** | <1% avec retry |
| **Overhead coordination** | <10% du temps total |
| **Scalabilit√©** | Lin√©aire jusqu'√† 10 workers |

---

## üîç Risques et mitigation

| Risque | Impact | Probabilit√© | Mitigation |
|--------|--------|-------------|------------|
| Contention GPU sur workers | √âlev√© | Moyenne | 1 t√¢che/worker, CUDA_VISIBLE_DEVICES |
| √âchec synchronisation checkpoints | √âlev√© | Faible | Redis locks + validation |
| Saturation r√©seau (NFS) | Moyen | Moyenne | Compression audio, batch uploads |
| Complexit√© debugging distribu√© | Moyen | √âlev√©e | Logs centralis√©s, tracing distribu√© |
| Co√ªt infrastructure | Faible | √âlev√©e | Auto-scaling, mode hybride cloud |

---

## üí° Am√©liorations futures

1. **Auto-scaling dynamique** : Ajouter/retirer workers selon la charge
2. **Support cloud natif** : AWS ECS/Fargate, K8s
3. **Optimisation r√©seau** : Compression audio en transit
4. **Pr√©-chargement intelligent** : Pr√©dire chapitres suivants
5. **Multi-tenancy** : Plusieurs livres simultan√©s sur m√™me cluster

---

## üìû Support et contribution

- **Issues** : GitHub Issues pour bugs
- **Discussions** : GitHub Discussions pour questions
- **PR** : Contributions bienvenues !

---

**Date de cr√©ation** : 2025-11-06
**Auteur** : Claude (Assistant IA)
**Version** : 1.0 - Plan initial
